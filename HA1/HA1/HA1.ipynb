{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a9946af93d6691886d4b3531b8b39d0",
     "grade": false,
     "grade_id": "cell-5690119ead85e67e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Checklist for submission\n",
    "\n",
    "Please check that:\n",
    "1. Everything runs as expected (no bugs when running cells);\n",
    "2. The output from each cell corresponds to its code (don't change any cell's contents without rerunning it afterwards);\n",
    "3. All outputs are present (don't delete any of the outputs);\n",
    "4. Fill in all the places that say `# YOUR CODE HERE`, or \"**Your answer:** (fill in here)\".\n",
    "5. Never copy/paste any notebook cells. Inserting new cells is allowed, but it should not be necessary.\n",
    "6. The notebook contains some hidden metadata which is important during our grading process. **Make sure not to corrupt any of this metadata!** The metadata may for example be corrupted if you copy/paste any notebook cells, or if you perform an unsuccessful git merge / git pull. It may also be pruned completely if using Google Colab, so watch out for this. Searching for \"nbgrader\" when opening the notebook in a text editor should take you to the important metadata entries.\n",
    "7. Although we will try our very best to avoid this, it may happen that bugs are found after an assignment is released, and that we will publish an updated version of the assignment. If this happens, it is important that you update to the new version, while making sure the notebook metadata is properly updated as well. The safest way to make sure nothing gets messed up is to start from scratch on a clean updated version of the notebook, copy/pasting your code from the cells of the previous version into the cells of the new version.\n",
    "\n",
    "We advise you to perform the following steps before submission to ensure that requirements 1, 2, and 3 are always met: **Restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). This might require a bit of time, so plan ahead for this. Finally press the \"Save and Checkpoint\" button before handing in, to make sure that all your changes are saved to this .ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a025ba528a4e9c11fc54be126fdffab0",
     "grade": false,
     "grade_id": "cell-5676bcf768a7f9be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in group number and member names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME1 = \"\" \n",
    "NAME2 = \"\"\n",
    "GROUP = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f0dd7cbad727dec0308b03071cff6d79",
     "grade": false,
     "grade_id": "cell-8092c3fd452a3245",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# HA1 - Cats and dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a54241ea89512f794ee9e366f2ef92f3",
     "grade": false,
     "grade_id": "cell-0235e816fc98b0f6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<img src=\"https://cdn.pixabay.com/photo/2015/05/20/10/03/cat-and-dog-775116_960_720.jpg\" alt=\"Image of cats and dogs\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8968e769197d5f393e659fe7fb1b16e6",
     "grade": false,
     "grade_id": "cell-c4bb694612153106",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "For this home assignment, we'll use the Kaggle dataset for the [Dogs vs. Cats competition](https://www.kaggle.com/c/dogs-vs-cats). It is comprised of 25k colour images of dogs and cats. Our goal with this assignment will be to create a classifier that can discriminate between cats or dogs.\n",
    "\n",
    "The goal is to make sure that you all can independently create, train and evaluate a model using a popular deep learning framework. A secondary goal is also to expose you to GPU computing, either your own or via a cloud computing service. The focus is on implementing the models, and much of the surrounding code is provided for you. You are expected to understand the provided code.\n",
    "\n",
    "From the survey done before the module, it is evident that the experience in both deep learning and Python programming vary greatly across the group, making the construction of a relevant assignment challenging. We err on the side of caution and try not to gloss over any Python peculiarities and to give some guidance toward best practices. For you who find this overly simple, we have left some parts of the assignment very flexible, so that you can experiment more freely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b29508e55eeb56eb4a4ff26097506772",
     "grade": false,
     "grade_id": "cell-ee9e2aee031325a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Using your cloud GPU\n",
    "\n",
    "### Strong recommendation:\n",
    "In order to make the most out of your GPU hours, first try solving the initial part of this notebook (tasks 0-3) in your own computer (these tasks can be solved on the CPU), and leave most of the available hours for solving tasks 4-5, and refining your best model further (and, if you have the spare hours, experiment a bit!).\n",
    "\n",
    "### Working efficiently:\n",
    "Training for several epochs just to have your code break at the last validation step is incredibly frustrating and inefficient. Good practice is to first test long training runs with a much simpler dry-run: a single epoch, a few batches et c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb68ccaeb7bda083faed92e52b3c8a10",
     "grade": false,
     "grade_id": "cell-f7371c24b57c153e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Requirements:\n",
    "- Whenever we ask you to plot anything, be sure to add a title and label the axes. If you're plotting more than one curve in the same plot, also add a legend.\n",
    "- When we ask you to train an architecture, train it for a reasonable number of epochs. \"Reasonable\" here means you should be fairly confident that training for a higher number of epochs wouldn't impact your conclusions regarding the model's performance. When experimenting, a single epoch is often enough to tell whether your model setup has improved or not.\n",
    "\n",
    "\n",
    "Hints:\n",
    "- Some implementation tasks come with automated tests, these are to guide you towards correct code, but there might be some false negatives or they may be non-applicable. If there is some issue with the test and you believe your code is correct, simply ignore the test.\n",
    "- If you get errors saying you've exhausted the GPU resources, well, then you've exhausted the GPU resources. However, sometimes that's because Pytorch didn't release a part of the GPU's memory. If you think your CNN should fit in your memory during training, try restarting the kernel and directly training only that architecture.\n",
    "- Every group has enough cloud credits to complete this assignment. However, this statement assumes you'll use your resources judiciously (e.g. always try the code first in your machine and make sure everything works properly before starting your instances) and **won't forget to stop your instance after using it,**  otherwise you might run out of credits.\n",
    "- Before starting, take a look at the images we'll be using. This is a hard task, don't get discouraged if your first models perform poorly (several participants in the original competition didn't achieve an accuracy higher than 60%).\n",
    "- Solving the computer labs and individual home assignments is a good way to get prepared for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f50e27a30d83bcebeb52a8ae43228e2",
     "grade": false,
     "grade_id": "cell-3ee6d24346a80d85",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 0. Imports\n",
    "\n",
    "In the following cell, add all the imports you'll use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0075c816ac7a24f2287d6fa9b8a81565",
     "grade": true,
     "grade_id": "cell-464a08ede00083a4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba7ff095a4e9cfaa58060479571801db",
     "grade": false,
     "grade_id": "cell-4821dc273028d702",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 1. Loading the data and preprocessing\n",
    "\n",
    "In this part we will set up the data used in this assignment. You need to download it, then we'll walk you through how to make a custom Pytorch dataset abstraction. The abstraction enables you to visualise and play around with the image data and to finally create dataloaders, which are necessary for the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "240150ea29723629712376346fa996f0",
     "grade": false,
     "grade_id": "cell-2ea049dea4713494",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The first step is to head to the [Kaggle website for the cats and dogs competition](https://www.kaggle.com/c/dogs-vs-cats/data) and download the data from there. You should download both the test and train folders together in one zip file (there is a `Download all` button at the bottom of the page). Unfortunately, you need to create a Kaggle account for this.\n",
    "\n",
    "**Only necessary for tasks 4-6**: Downloading the data to your local computer is quite straight-forward. Sooner or later you will have to upload the data to the cloud instance and that is a bit more tricky. There are a few ways to do it:\n",
    "\n",
    " - Jupyter Notebook upload function. When starting the notebook server with the command `jupyter notebook` you are directed to a main page. In the top right corner there is an upload button.\n",
    " - Using [`scp`](https://linuxize.com/post/how-to-use-scp-command-to-securely-transfer-files/) to copy files via an ssh connection.\n",
    " - Using the [Kaggle CLI](https://github.com/Kaggle/kaggle-api). We have added it to the conda environment.\n",
    "\n",
    "To begin with, download the data to your local computer and create a folder structure that resembles the following (obviously, the folder names are up to you):\n",
    "\n",
    "\n",
    "         small_train             small_val                train                   val\n",
    "              |                      |                      |                      |\n",
    "              |                      |                      |                      |\n",
    "        -------------          -------------          -------------          -------------\n",
    "        |           |          |           |          |           |          |           |\n",
    "        |           |          |           |          |           |          |           |\n",
    "      cats        dogs       cats        dogs       cats        dogs       cats        dogs\n",
    "\n",
    "\n",
    "The `small_train` and `small_val` folders have the training and validation samples for your smaller subset of the data, while the `train` and `val` folders contain all the samples you extracted from Kaggle's `train.zip`.\n",
    "This is just a convenient way of having a smaller dataset to play with for faster prototyping.\n",
    "\n",
    "We provide you a notebook that shows how to achieve this folder structure (`create_project_notebook_structure.ipynb`), starting from the original `dogs-vs-cats.zip` file that you download from Kaggle. If you do use that notebook, we encourage you to understand how each step is being done, so you can generalize this knowledge to new datasets you'll encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d587c818e2dee1a216947ba21f42d528",
     "grade": false,
     "grade_id": "cell-89ba19509b952af2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "For the smaller dataset, we advise you to use 70% of the data as training data (and thereby the remaining 30% for validation data). However, for the larger dataset, you should decide how to split between training and validation.\n",
    "\n",
    "**What percentage of the larger dataset did you decide to use for training?**\n",
    "\n",
    "**Did you decide to keep the same ratio split between train and validation sets for the larger dataset? Motivate your decision!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c78fb7af1d8b50d9607cf49e87d1ce51",
     "grade": true,
     "grade_id": "cell-7f3b0dfbd90a14c1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52446f69c5a2d21b981ac5a938f1c010",
     "grade": false,
     "grade_id": "cell-876ca7df88c9311f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Fill in the dataset paths (to be used later by your dataloaders):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "977d3723abd484be4bfa4ffb4590a2d1",
     "grade": true,
     "grade_id": "cell-1b1314f2ab1b1d6b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Change the directories accordingly\n",
    "train_path = \"/your/path\"\n",
    "val_path = \"/your/path\"\n",
    "small_train_path = \"/your/path\"\n",
    "small_val_path = \"/your/path\"\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "701daf5869bcd81609ed5c012432bedb",
     "grade": false,
     "grade_id": "cell-1d6ea64bca94a4ef",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 1.1 Dataset\n",
    "\n",
    "To create dataloaders we first need to create a dataset abstraction class. The purpose of a dataloader is to efficiently provide the CPU/GPU with mini-batches of data. We now work with data complex enough to actually warrant the use of dataloaders. In particular, we don't want to load all images into memory at once.\n",
    "\n",
    "Like before, the dataloader is an instance of the Pytorch [`DataLoader`](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader) which wraps a class that inherits from [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset), that we must create.\n",
    "Image classficiation is such a common task that Pytorch provides a ready-to-use dataset class for this task, called [`ImageFolder`](https://pytorch.org/vision/stable/datasets.html?highlight=imagefolder#imagefolder).\n",
    "Using this class however, is rather opaque so for your understanding we will show you how to construct a custom dataset class. If you know this method, you will be able to create a dataloader for any dataset you may encounter.\n",
    "\n",
    "We construct a class `DogsCatsData` as a subclass of `Dataset`. \n",
    "The dataset subclass holds the actual data, or at least provides access to it.\n",
    "To make it work with the `DataLoader` class we need to implement two methods:\n",
    "\n",
    "- `__getitem__(self, index)`: return the `index`'th sample, i.e. a single pair of (image, label)\n",
    "- `__len__(self)`: simply return the total number of samples $N$ in the dataset.\n",
    "\n",
    "These methods are so called Python \"magic\" methods, signified by the leading and closing double underscores.\n",
    "They typically enable special syntax for a class: `__getitem__` enables indexing of a class, and `__len__` enables calling the `len` function:\n",
    "```python\n",
    "# Consider an instance `data` of a class `MyDataset` implementing `__getitem__` and `__len__`\n",
    "data[10] # returns the 10th item in `data`\n",
    "len(data) # returns the length/size of `data`\n",
    "```\n",
    "We will return to why these are needed in the `DataLoader` wrapping class\n",
    "\n",
    "Now, to the actual implementation: The idea is to have the dataset class only store the filenames of the images (and the corresponding label), not the images themselves. We will find and store the filenames in the constructor. The `__getitem__` method will use the index to look up the correct filename and load it into memory.\n",
    "The `__len__` method is trivial to implement, we need only count the stored filenames.\n",
    "\n",
    "Take a look at the provided code, make sure you understand what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "class DogsCatsData(Dataset):\n",
    "    \n",
    "    def __init__(self, root, transform, dog_label=1, cat_label=0):\n",
    "        \"\"\"Constructor\n",
    "        \n",
    "        Args:\n",
    "            root (Path/str): Filepath to the data root, e.g. './small_train'\n",
    "            transform (Compose): A composition of image transforms, see below.\n",
    "        \"\"\"\n",
    "        \n",
    "        root = Path(root)\n",
    "        if not (root.exists() and root.is_dir()):\n",
    "            raise ValueError(f\"Data root '{root}' is invalid\")\n",
    "            \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self._dog_label = 1\n",
    "        self._cat_label = 0\n",
    "        \n",
    "        # Collect samples, both cat and dog and store pairs of (filepath, label) in a simple list.\n",
    "        self._samples = self._collect_samples()\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get sample by index\n",
    "        \n",
    "        Args:\n",
    "            index (int)\n",
    "        \n",
    "        Returns:\n",
    "             The index'th sample (Tensor, int)\n",
    "        \"\"\"\n",
    "        # Access the stored path and label for the correct index\n",
    "        path, label = self._samples[index]\n",
    "        # Load the image into memory\n",
    "        img = Image.open(path)\n",
    "        # Perform transforms, if any.\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Total number of samples\"\"\"\n",
    "        return len(self._samples)\n",
    "    \n",
    "    def _collect_samples(self):\n",
    "        \"\"\"Collect all paths and labels\n",
    "        \n",
    "        Helper method for the constructor\n",
    "        \"\"\"\n",
    "        # Iterator over dog filpath\n",
    "        dog_paths = self._collect_imgs_sub_dir(self.root / \"dogs\")\n",
    "        # Iterator of pairs (path, dog label)\n",
    "        # Again, we use the `map` function to create an iterator. It's use is not as common as the so called\n",
    "        # 'list comprehension' you've previously seen, but a good alternative to have seen.\n",
    "        dog_paths_and_labels = map(lambda path: (path, self._dog_label), dog_paths)\n",
    "        # Same for cats\n",
    "        cat_paths = self._collect_imgs_sub_dir(self.root / \"cats\")\n",
    "        cat_paths_and_labels = map(lambda path: (path, self._cat_label), cat_paths)\n",
    "        # Sorting is not strictly necessary, but filesystem globbing (wildcard search) is not deterministic,\n",
    "        # and consistency is nice when debugging.\n",
    "        return sorted(list(chain(dog_paths_and_labels, cat_paths_and_labels)), key=lambda x: x[0].stem)\n",
    "     \n",
    "    @staticmethod\n",
    "    def _collect_imgs_sub_dir(sub_dir: Path):\n",
    "        \"\"\"Collect image paths in a directory\n",
    "        \n",
    "        Helper method for the constructor\n",
    "        \"\"\"\n",
    "        if not sub_dir.exists():\n",
    "            raise ValueError(f\"Data root '{self.root}' must contain sub dir '{sub_dir.name}'\")\n",
    "        return sub_dir.glob(\"*.jpg\")\n",
    "    \n",
    "    def get_sample_by_id(self, id_):\n",
    "        \"\"\"Get sample by image id\n",
    "        \n",
    "        Convenience method for exploration.\n",
    "        The indices does not correspond to the image id's in the filenames.\n",
    "        Here is a (rather inefficient) way of inspecting a specific image.\n",
    "        \n",
    "        Args:\n",
    "            id_ (str): Image id, e.g. `dog.321`\n",
    "        \"\"\"\n",
    "        id_index = [path.stem for (path, _) in self._samples].index(id_)\n",
    "        return self[id_index]\n",
    "\n",
    "# Example usage:\n",
    "    \n",
    "img_width, img_height = 224, 224\n",
    "transformations = Compose([ToTensor()])\n",
    "example_dataset = DogsCatsData(Path.cwd() / \"small_train\", transform=transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32afdc1ceb5019325bbeee55c8f4cddf",
     "grade": false,
     "grade_id": "cell-e03f109baaddeb95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It is vital to explore your data, but it can be tricky to deal with images in the tensor format.\n",
    "To aid you, use the below helper function to visually inspect your images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(axis, image_tensor):\n",
    "    \"\"\"Display a tensor as an image\n",
    "    \n",
    "    Args:\n",
    "        axis (pyplot axis)\n",
    "        image_tensor (torch.Tensor): tensor with shape (num_channels=3, width, heigth)\n",
    "    \"\"\"\n",
    "    \n",
    "    # See hint above\n",
    "    if not isinstance(image_tensor, torch.Tensor):\n",
    "        raise TypeError(\"The `display_image` function expects a `torch.Tensor` \" +\n",
    "                        \"use the `ToTensor` transformation to convert the images to tensors.\")\n",
    "        \n",
    "    # The imshow commands expects a `numpy array` with shape (3, width, height)\n",
    "    # We rearrange the dimensions with `permute` and then convert it to `numpy`\n",
    "    image_data = image_tensor.permute(1, 2, 0).numpy()\n",
    "    height, width, _ = image_data.shape\n",
    "    axis.imshow(image_data)\n",
    "    axis.set_xlim(0, width)\n",
    "    # By convention when working with images, the origin is at the top left corner.\n",
    "    # Therefore, we switch the order of the y limits.\n",
    "    axis.set_ylim(height, 0)\n",
    "\n",
    "# Example usageprocess\n",
    "_, axis = plt.subplots()\n",
    "some_random_index = 453\n",
    "# Here we use the __getitem__ method as a \"magic\" method.\n",
    "# Implementing it for a class, enables square bracket '[]' indexing\n",
    "image_tensor, label = example_dataset[some_random_index]\n",
    "display_image(axis, image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preprocessing \n",
    "The `DogsCatsData` class constructor has an argument called `transform`. It allows us to transform or preprocess all the images in a batch, from the raw image data to a more suitable format. There are multiple motivations for preprocessing:\n",
    "\n",
    "- Some transformations might be needed to actually make the data work with our network (reshaping, permuting dimensions et c.).\n",
    "- Make the training more efficient by making the input dimensions smaller, e.g. resizing, cropping.\n",
    "- Artificially expanding the training data through [data augmentation](https://cartesianfaith.com/2016/10/06/what-you-need-to-know-about-data-augmentation-for-machine-learning/)\n",
    "- We have some clever idea of how to change the data to create a simpler optimisation problem.\n",
    "\n",
    "We do not expect you to do data augmentation, but feel free to preprocess the data as you see fit. Use the [documentation](https://pytorch.org/vision/stable/transforms.html#torchvision-transforms) to view available transforms. Extra important is the `Compose` transformation, which is a meta-transformation which composes actual ones, and the `ToTensor` transformation which is the simplest way to go from image to tensor format.\n",
    "\n",
    "\n",
    "Hints:\n",
    "- Revisit the `DogsCatsData` example usage to see how to use the `Compose` transformation.\n",
    "- When feeding the images to your CNN, you'll probably want all of them to have the same spatial size, even though the .jpeg files differ in this. Resizing the images can be done using the previously mentioned Pytorch Transforms.\n",
    "- Resizing the images to a smaller size while loading them can be beneficial as it speeds up training. The CNN's do surprisingly well on 64x64 or even 32x32 images. Shorter training cycles give you more time to experiment! Note: The VGG network used later in this assignment is specialised for images that are 224x224.\n",
    "\n",
    "We encourage you to explore the data and choose transformations that you believe to be useful. For exploration we provide you with a helper function to visually compare transformations side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49aaab88c887daaa4d34a35ac2837e09",
     "grade": false,
     "grade_id": "cell-5ca8fc808d4ee65b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compare_transforms(transformations, index):\n",
    "    \"\"\"Visually compare transformations side by side.\n",
    "    Takes a list of DogsCatsData datasets with different compositions of transformations.\n",
    "    It then display the `index`th image of the dataset for each transformed dataset in the list.\n",
    "    \n",
    "    Example usage:\n",
    "        compare_transforms([dataset_with_transform_1, dataset_with_transform_2], 0)\n",
    "    \n",
    "    Args:\n",
    "        transformations (list(DogsCatsData)): list of dataset instances with different transformations\n",
    "        index (int): Index of the sample in the dataset you wish to compare.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we combine two functions from basic python to validate the input to the function:\n",
    "    # - `all` takes an iterable (something we can loop over, e.g. a list) of booleans\n",
    "    #    and returns True if every element is True, otherwise it returns False.\n",
    "    # - `isinstance` checks whether a variable is an instance of a particular type (class)\n",
    "    if not all(isinstance(transf, Dataset) for transf in transformations):\n",
    "        raise TypeError(\"All elements in the `transformations` list need to be of type Dataset\")\n",
    "        \n",
    "    num_transformations = len(transformations)\n",
    "    fig, axes = plt.subplots(1, num_transformations)\n",
    "    \n",
    "    # This is just a hack to make sure that `axes` is a list of the same length as `transformations`.\n",
    "    # If we only have one element in the list, `plt.subplots` will not create a list of a single axis\n",
    "    # but rather just an axis without a list.\n",
    "    if num_transformations == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for counter, (axis, transf) in enumerate(zip(axes, transformations)):\n",
    "        axis.set_title(\"transf: {}\".format(counter))\n",
    "        image_tensor = transf[index][0]\n",
    "        display_image(axis, image_tensor)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0be552e71693f765763bfc62edeb222",
     "grade": true,
     "grade_id": "cell-31b81f052b6e681e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Explore your dataset in this cell, you do not need to produce any results.\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf7edd8e56875bd791419d33ae2b87da",
     "grade": false,
     "grade_id": "cell-24463974ae20a276",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Normalisation of the training data is popular in pre-processing. What is the argument or intuition for why this is a beneficial transformation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd23a25cf927b4737fd1f2935028ea56",
     "grade": true,
     "grade_id": "cell-6509545d160bac4b",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "399527b15aa347fc65dc4803bb91d677",
     "grade": false,
     "grade_id": "cell-5553a56f43c9298a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 Data loaders\n",
    "With our dataset class implementation in place, creating a `DataLoader` instance is simple.\n",
    "\n",
    "The dataloader class wraps the dataset and provides a way to iterate over batches in the training loop.\n",
    "To produce batches, it gets the total number of samples $N$ with the dataset's `__len__` method.\n",
    "It divides the indices $1, \\dots, N$ into equally sized index batches with $B$ (batch size) elements. A particular batch with pairs of image and label is created by calling the dataset's `__getitem__` method with the indices in the batch. NB: the last batch in an epoch might be smaller if $N$ is not divisible by $B$.\n",
    "\n",
    "Create the data loaders needed for training (use the small version of the data), in the cell below.\n",
    "The `DataLoader` class is documented [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) , but it's not that much to it.\n",
    "You simply create a dataloader with a dataset instance and some other (self-explanatoty) settings:\n",
    "\n",
    "```python\n",
    "train_dataloader = DataLoader(example_dataset, batch_size=batch_size, shuffle=True)\n",
    "```\n",
    "\n",
    "Create data loaders required for training and validation.\n",
    "\n",
    "Hints:\n",
    "- The specified `batch_size` should be chosen so that you train fast but don't run out of memory. You need to figure this out empirically; start small and increase the batch size until you run out of memory. Beyond this pragmatic approach, feel free to contribute to the highly contested relation between batch size and generalisation.\n",
    "- The `DataLoader` constructor takes an optional argument `num_workers`, which defaults to `0` if not provided. Setting a higher number creates multiple threads which load batches concurrently. This can speed up training considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc3a785728de7309ed963b9e83c06623",
     "grade": true,
     "grade_id": "cell-051ee24a83af3cf8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    " # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5091f03d2c006cdca9db8c660226ce7a",
     "grade": false,
     "grade_id": "cell-c0bfc1ac7fadfcc7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 2. Training\n",
    "\n",
    "### 2.1 The first model\n",
    "\n",
    "Now, it's time to create a model called `FirstCnn`. To begin with, you have to create a CNN to an exact specification. After that, you will get the chance to be more creative.\n",
    "\n",
    "For the first model, create a network that:\n",
    "- Inherits from [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module)\n",
    "- Implements a constructor `__init__(self, img_size)`, a `forward(self, input_batch)` method and whatever other helper methods you deem necessary. Note that a (square) image size should be a parameter in the model's constructor. While not strictly necessary, it is an acceptable way of handling varying input dim's and it is convenient for testing.\n",
    "- Can handle square images of arbitrary size and arbitrary large mini-batch sizes (within memory limits, of course). You may assume that there are always three colour channels, i.e., a mini-batch will have the shape `(batch size = B, num channels = 3, img size = D, D)`\n",
    "- Has layers:\n",
    "    1. Two convolutional layers, each with 10 filters, kernel size = 3, stride = 1, padding = 0\n",
    "    2. A single fully connected layer.\n",
    "- Related layers such as a pooling operation are optional.\n",
    "- Outputs the probability of the image belonging to the class 'dog'. Technically the output should consist of `B` probabilities, one for each image in the mini-batch and so have the shape `(B,)`.\n",
    "\n",
    "Hints:\n",
    "\n",
    "- The subpage for [`torch.nn`](https://pytorch.org/docs/stable/nn.html) is a good place to find the layer specifics.\n",
    "- Going from the last CNN layer to the final fully connected layer is not trivial. The convolutions produces feature maps which we can think of as an image with many channels, while the fully connected layer expects a row vector as input. Calculate how many output neurons the convolutions produce and use `.reshape` to make your tensor fit the fully connected layer. It is also common to see the `.view` and `.squeeze` methods to do the same thing. They basically do the same thing (apart from some differences in internal memory management) but are less transparent. *Hint within the hint:* remember that the fully connected layers expects a *batch* of 1D tensors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2640d61f483bd3ac9c018fd407f71ace",
     "grade": true,
     "grade_id": "cell-4c9de348cd8bc4ff",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_class):\n",
    "    assert issubclass(model_class, nn.Module), \"Model class should inherit from nn.Module\"\n",
    "    \n",
    "    assert getattr(model_class, \"forward\", None) is not None, \"Model class should have a 'forward' method\"\n",
    "    \n",
    "    _test_output(model_class, some_img_size=224, some_batch_size=64)\n",
    "    _test_output(model_class, some_img_size=32, some_batch_size=8)\n",
    "    \n",
    "    print(\"Test passed.\")\n",
    "    \n",
    "def _test_output(model_class, some_img_size, some_batch_size):\n",
    "    random_input = torch.rand(some_batch_size, 3, some_img_size, some_img_size)\n",
    "    model_instance = model_class(img_size=some_img_size)\n",
    "    output = model_instance.forward(random_input)\n",
    "    output_shape = list(output.shape)\n",
    "    assert output_shape == [some_batch_size], f\"Expected output size [{some_batch_size}], got {output_shape}\"\n",
    "    \n",
    "# Note that the test takes the actual class, not an instance of it, as input.\n",
    "# Here, we assume that the model class is named 'FirstCnn'\n",
    "test_model(FirstCnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc0b3040e428bbbd4e670c31cae75ef4",
     "grade": false,
     "grade_id": "cell-12ad4c02e8150588",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You have been told that one of the benefits of CNN is that it can handle input of different sizes. Yet, you needed to know the image size in the constructor.\n",
    "Explain how you made your model handle different input sizes and why it is necessary, despite it being a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef73df1f055b5a6add785d6ad4c207e8",
     "grade": true,
     "grade_id": "cell-c06d2ae30ee4c649",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d78df39e3aacc7ec67ec4c29c67754c1",
     "grade": false,
     "grade_id": "cell-cb6fc78116ad6b75",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.2 The training loop\n",
    "\n",
    "You have already seen quite a few training loops in the preparations. Below we provide you with an example of a basic one that you can use if you please.\n",
    "If you do use it, you need to provide an implementation that maps network outputs (probabilites) to hard labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4238be00a431e45401c9f1283bccef2",
     "grade": true,
     "grade_id": "cell-4b4d93eec45833cf",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def output_to_label(z):\n",
    "    \"\"\"Map network output z to a hard label {0, 1}\n",
    "    \n",
    "    z (Tensor): Probabilities for each sample in a batch.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "def test_output_to_label(fn):\n",
    "    batch_size = torch.randint(1, 64, (1,))\n",
    "    random_logits = torch.rand(batch_size)\n",
    "    random_probs = random_logits / random_logits.sum()\n",
    "    labels = fn(random_probs)\n",
    "    assert labels.shape == random_logits.shape, \"The element-wise function should preserve the shape\"\n",
    "    assert labels.dtype == torch.long, \"Incorrect datatype, should be torch.long\"\n",
    "    fixed_logits = torch.tensor([0.1, 0.9, 0.51, 0.49, 0.7])\n",
    "    fixed_labels = torch.tensor([0, 1, 1, 0, 1], dtype=torch.long)\n",
    "    assert all(fixed_labels == fn(fixed_logits)), \"Incorrect fixed output\"\n",
    "    \n",
    "\n",
    "test_output_to_label(output_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every):\n",
    "    print(\"Starting training\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model, train_loss, train_acc = train_epoch(model,\n",
    "                                                   optimizer,\n",
    "                                                   loss_fn,\n",
    "                                                   train_loader,\n",
    "                                                   val_loader,\n",
    "                                                   device)\n",
    "        val_loss, val_acc = validate(model, loss_fn, val_loader, device)\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "              f\"Train loss: {sum(train_loss)/len(train_loss):.3f}, \"\n",
    "              f\"Train acc.: {sum(train_acc)/len(train_acc):.3f}, \"\n",
    "              f\"Val. loss: {val_loss:.3f}, \"\n",
    "              f\"Val. acc.: {val_acc:.3f}\")\n",
    "        train_losses.extend(train_loss)\n",
    "        train_accs.extend(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "    return model, train_losses, train_accs, val_losses, val_accs\n",
    "\n",
    "def train_epoch(model, optimizer, loss_fn, train_loader, val_loader, device):\n",
    "    # Train:\n",
    "    train_loss_batches, train_acc_batches = [], []\n",
    "    num_batches = len(train_dataloader)\n",
    "    for batch_index, (x, y) in enumerate(train_loader, 1):\n",
    "        inputs, labels = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = model.forward(inputs)\n",
    "        loss = loss_fn(z, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_batches.append(loss.item())\n",
    "\n",
    "        hard_preds = output_to_label(z)\n",
    "        acc_batch_avg = (hard_preds == labels).float().mean().item()\n",
    "        train_acc_batches.append(acc_batch_avg)\n",
    "\n",
    "        # If you want to print your progress more often than every epoch you can\n",
    "        # set `print_every` to the number of batches you want between every status update.\n",
    "        # Note that the print out will trigger a full validation on the full val. set => slows down training\n",
    "        if print_every is not None and batch_index % print_every == 0:\n",
    "            val_loss, val_acc = validate(model, loss_fn, val_loader, device)\n",
    "            model.train()\n",
    "            print(f\"\\tBatch {batch_index}/{num_batches}: \"\n",
    "                  f\"\\tTrain loss: {sum(train_loss_batches[-print_every:])/print_every:.3f}, \"\n",
    "                  f\"\\tTrain acc.: {sum(train_acc_batches[-print_every:])/print_every:.3f}, \"\n",
    "                  f\"\\tVal. loss: {val_loss:.3f}, \"\n",
    "                  f\"\\tVal. acc.: {val_acc:.3f}\")\n",
    "\n",
    "    return model, train_loss_batches, train_acc_batches\n",
    "\n",
    "def validate(model, loss_fn, val_loader, device):\n",
    "    val_loss_cum = 0\n",
    "    val_acc_cum = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (x, y) in enumerate(val_loader, 1):\n",
    "            inputs, labels = x.to(device), y.to(device)\n",
    "            z = model.forward(inputs)\n",
    "\n",
    "            batch_loss = loss_fn(z, labels.float())\n",
    "            val_loss_cum += batch_loss.item()\n",
    "            hard_preds = output_to_label(z)\n",
    "            acc_batch_avg = (hard_preds == labels).float().mean().item()\n",
    "            val_acc_cum += acc_batch_avg\n",
    "    return val_loss_cum/len(val_dataloader), val_acc_cum/len(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af30b242914b18e73e344510400e8dc6",
     "grade": false,
     "grade_id": "cell-3a2bedde765aa366",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the training loop to train your model, using the two data loaders you created earlier. Train for a reasonable amount of epochs, so as to get a good sense of how well this architecture performs.\n",
    "\n",
    "Hints:\n",
    "- Training on a CPU is slow and in the beginning you just want to verify that your architecture actually produces a predicition with the correct shape. Make everything you can to speed up the prototyping phase, e.g. train only for a single epoch and make the images ridiculously small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff72f3c30e54fed1b214aadf6dacaa28",
     "grade": true,
     "grade_id": "cell-0b7b0d0139ba368a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b83618d5fa70d38ba734f89f0b2916f1",
     "grade": false,
     "grade_id": "cell-4d42c86687697a67",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.3 Visualisation\n",
    "\n",
    "Create two plots. In one of them, plot the loss in the training and the validation datasets. In the other one, plot the accuracy in the training and validation datasets.\n",
    "Note that the given training loop produces metrics at different intervals for training and validation, make sure that you align your metrics in a way that makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e71729e832321272983178b41b90f874",
     "grade": true,
     "grade_id": "cell-fa81712e1e27432a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5797f9d2d079b67832722cebe8cff0ef",
     "grade": false,
     "grade_id": "cell-f2fc166890962bcf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Based on these, what would you suggest for improving your model? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b42c08ae7bcb61726d89b8dd46922d6",
     "grade": true,
     "grade_id": "cell-506e21ce469b67f5",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afa8e2c066a79d25a5ad58e8095cbfac",
     "grade": false,
     "grade_id": "cell-ee79a83a62b70a8f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 3. Improving your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29794d27a03330f15d689d455a5adc53",
     "grade": false,
     "grade_id": "cell-5314d286e79e0377",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now you are free to create whichever model you want. A simple improvement based on your analysis of the above results is fine, or you can do something more ambitious. When you're happy with one architecture, copy it in the cell below and train it here. Save the training and validation losses and accuracies. You'll use this later to compare your best model with the one using transfer learning.\n",
    "\n",
    "**Note**: When trying different ideas, you'll end up with several different models. However, when submitting your solutions to Canvas, the cell below must contain only the definition and training of *one* model. Remove all code related to the models that were not chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45e0af4b6ac1d453b519ba7c65ccb56d",
     "grade": true,
     "grade_id": "cell-6edb7d7e343ab14b",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba6a43ffd69fce436811d9311aedcede",
     "grade": false,
     "grade_id": "cell-d033937b5a8b9875",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Create two plots. In one of them, plot the loss in the training and the validation datasets. In the other one, plot the accuracy in the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88f944698dc9dc353e1933fe16b6de87",
     "grade": true,
     "grade_id": "cell-3df999674672de47",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c12d911c8d5d26764f8af9f6904a3e1",
     "grade": false,
     "grade_id": "cell-a827c39d9e652e52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Did your results improve? What problems did your improvements fix? Explain why, or why not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "951f6d4450c82df3800e38edd678a422",
     "grade": true,
     "grade_id": "cell-cbda4b585ad39ddc",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c06074522725240265e2182d83fda4e2",
     "grade": false,
     "grade_id": "cell-c67bcc4fbec1808e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Save your model](https://pytorch.org/tutorials/beginner/saving_loading_models.html) to disk (the architecture, weights and optimizer state). This is simply so you can use it again easily in the later parts of the notebook, without having to keep it in memory or re-training it. The actual file you create is not relevant to your submission. The code to save the model is given in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that you called your model \"my_model\"\n",
    "torch.save(model.state_dict(), \"my_model\")\n",
    "# Create and initialise model with a previously saved state dict:\n",
    "saved_model = FirstCnn(img_size=224)\n",
    "saved_model.load_state_dict(torch.load(\"my_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ceb570afa39d746b3b5f132ecb5bc72e",
     "grade": false,
     "grade_id": "cell-25f9cc8d17491d0d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 4. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "209c5d113dd1805f4e44dbebc1192473",
     "grade": false,
     "grade_id": "cell-cf9b347fc3ee9255",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**From now, training on CPU will not be feasible. If your computer has a GPU, try it out! Otherwise, now is the time to connect to your cloud instance**\n",
    "\n",
    "Now, instead of trying to come up with a good architecture for this task, we'll use the VGG16 architecture, but with the top layers removed (the fully connected layers + softmax). We'll substitute them with a single fully connected layer, and a classification layer that makes sense for our problem.\n",
    "\n",
    "However, this model has a very high capacity, and will probably suffer a lot from overfitting if we try to train it from scratch, using only our small subset of data. Instead, we'll start the optimization with the weights obtained after training VGG16 on the ImageNet dataset.\n",
    "\n",
    "Start by loading the *pretrained* VGG16 model, from the [torchvision.models](https://pytorch.org/docs/stable/torchvision/models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb3fdd07274fc1ec2d0638c0b5cf424c",
     "grade": false,
     "grade_id": "cell-01ebc4c9c306b985",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vgg_model = models.vgg16(pretrained=True)\n",
    "print(vgg_model.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f04d97f938d6f944741d657a0350e9da",
     "grade": false,
     "grade_id": "cell-faed8047ef25a60d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Create a new model with the layers you want to add on top of VGG.\n",
    "\n",
    "*Hint:*\n",
    "- You can access and modify the top layers of the VGG model with `vgg_model.classifier`, and the remaining layers with `vgg_model.features`.\n",
    "- You can get the number of output features of `vgg_model.features` with `vgg_model.classifier[0].in_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62829c2400b8ae993b346dd1a4ca68c0",
     "grade": true,
     "grade_id": "cell-56cb37360051a638",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e10f4a98f6a80e4e74c0d5bf18dbc5f",
     "grade": false,
     "grade_id": "cell-d746f9eb61e3ea44",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now add the new model on top of VGG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "642f3cbea497868385adff16643091c4",
     "grade": true,
     "grade_id": "cell-76e4aad7fbcf5d05",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ff2dfca08a9262327d0ee7d0c58b2bd",
     "grade": false,
     "grade_id": "cell-f76d1a7f6280af0d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 4.1 Using VGG features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9238f498830e5c8e53eab49c8ef3600",
     "grade": false,
     "grade_id": "cell-270f8ec140ddfba3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we're almost ready to train the new model. For transfer learning we want to freeze all but the top layers in your architecture (i.e. signal to the optimizer that the bottom layers should not be changed during optimization) by setting the attribute `requires_grad` for all parameters `vgg_model.features` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "072f414eabdbed6bd1f0baa8e855e48f",
     "grade": true,
     "grade_id": "cell-bfb58ea46c31df0a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d306e1a138f63e8c8c5eb90707013c9e",
     "grade": false,
     "grade_id": "cell-b508ede3d760a86b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Perform the transfer learning by training the top layers of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25b7eea0fa2f7c1cb7db636c5df1991e",
     "grade": true,
     "grade_id": "cell-f50c3d451530b9a8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34bf11c4a55f184ccc7d589c841b0be2",
     "grade": false,
     "grade_id": "cell-ad79e1aa5c4a6185",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Create two plots. In one of them, plot the loss in the training and the validation datasets. In the other one, plot the accuracy in the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99986b7bbdfb6b78c25112751969d11f",
     "grade": true,
     "grade_id": "cell-f17c882b2a09dee7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c4634579a40b62a3db715b027fbd80a",
     "grade": false,
     "grade_id": "cell-779d477ffe1ebbf6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "How does the model perform, compared to the model obtained in step 3? Create one plot with the training accuracy and another with the validation accuracy of the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "873c045fa2e6f22815a90194ed2785f3",
     "grade": true,
     "grade_id": "cell-e3e3990ba39bea67",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "435aa0252fc48c84f439bc5faf6bf18d",
     "grade": false,
     "grade_id": "cell-b84dd461d5ddcc8d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Compare these results. Which approach worked best, starting from scratch or doing transfer learning? Reflect on whether your comparison is fair or not:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dce3df56e420472eafb032186728064f",
     "grade": true,
     "grade_id": "cell-f9e1a6a643946cd2",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1d1fd0b9a00091e75a5bd0eaa19a8bf",
     "grade": false,
     "grade_id": "cell-c8afb448c67da5f8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "What are the main differences between the ImageNet dataset and the Dogs vs Cats dataset we used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e1152bb9b59ad5cb8016d3ebe648e97",
     "grade": true,
     "grade_id": "cell-2be321b63232ae01",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c0b0eae153b6076ca628a773203df42",
     "grade": false,
     "grade_id": "cell-71a8b8de004f6e57",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Even though there are considerable differences between these datasets, why is it that transfer learning is still a good idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9605a949f6c6b94fac8feb5a6bcbbcad",
     "grade": true,
     "grade_id": "cell-655d00face15a862",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d9425a067d87ef11d206088e82bb3c7",
     "grade": false,
     "grade_id": "cell-19785940b9624d2c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In which scenario would transfer learning be unsuitable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce818a8e86ceb511348597d975b34016",
     "grade": true,
     "grade_id": "cell-e79df7472ff5506a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b24258079f4e71e1842b78e479095117",
     "grade": false,
     "grade_id": "cell-111f2b1d28919293",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Save the model to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21141b44ffdb8df562dad77aa330661e",
     "grade": true,
     "grade_id": "cell-674350e34be30d10",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab5bf17534c2ac6852d79e32793fdbf7",
     "grade": false,
     "grade_id": "cell-544a73726bebe121",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 4.2 Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54080ee12529dc9ec0d2a887cc564243",
     "grade": false,
     "grade_id": "cell-1ee9ebc87fd3358e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now that we have a better starting point for the top layers, we can train the entire network. Unfreeze the bottom layers by resetting the `requires_grad` attribute to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b0054e10303716fdcbd286c57a19ece",
     "grade": true,
     "grade_id": "cell-3918c2cdd9817f7e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "37018e0e49c752230061f0f9fbed700c",
     "grade": false,
     "grade_id": "cell-80fa8c89f1b262f1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Fine tune the model by training all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71ce705f916d83ecec507903ac4a4c64",
     "grade": true,
     "grade_id": "cell-594c6039216461e5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de73952e2230136bd52c1c3b2c590896",
     "grade": false,
     "grade_id": "cell-5dc3e388a41da3ed",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "How does the model perform, compared to the model trained with frozen layers? Create one plot with the training accuracy and another with the validation accuracy of the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7b2e69e2ffc7f5bff07ba62225b4cee",
     "grade": true,
     "grade_id": "cell-7edb12ee397ec817",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "441147f03db17015e231b28be437fc06",
     "grade": false,
     "grade_id": "cell-5dae528a81d5ff24",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Why is it a good idea to use a very small learning rate when doing fine tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69dfca588131944b0e9825a1532de432",
     "grade": true,
     "grade_id": "cell-0f4a5edca490320e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4971aa0a2e159c1780dedfc5e78b7c15",
     "grade": false,
     "grade_id": "cell-4ed3967e4f6c5f7f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Save the model to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"trans_learning_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42d6362e7a0f25fc579ad6e33f1b401b",
     "grade": false,
     "grade_id": "cell-56908ee1e60aa411",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 4.3 Improving the top model (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f74ac0be60b7253bfe604c521647a07",
     "grade": false,
     "grade_id": "cell-3c8d8e5ab949ee35",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Improve the architecture for the layers you add on top of VGG16. Try different ideas! When you're happy with one architecture, copy it in the cell below and train it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c69b20551001d0e612f3b9221dc7dbc",
     "grade": true,
     "grade_id": "cell-22d09c8401d84b61",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9c7b7e5b3cbda8641216e260a75e3be",
     "grade": false,
     "grade_id": "cell-48933baad6c5afeb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "How does the model perform, compared to the model trained in step 4.2? Create one plot with the training accuracy and another with the validation accuracy of the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b520759c1ceb8218d203dc9655d25361",
     "grade": true,
     "grade_id": "cell-7cb62a04916a848e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84662189498e2454093c4a54a53716d6",
     "grade": false,
     "grade_id": "cell-8bbfa3e11e2dfff9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Save the model to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cec393e4f57e72ff91f5d55ad0dc14c",
     "grade": true,
     "grade_id": "cell-e64508c0fe4fa4f6",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49c722f31cfd70e995b6226c86584565",
     "grade": false,
     "grade_id": "cell-ad0efbac33de5a65",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 5. Final training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e460754d2c0f05f0e79ae982a3fe3d3",
     "grade": false,
     "grade_id": "cell-cf811afdac96843b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we'll train the model that achieved the best performance so far using the entire dataset.\n",
    "\n",
    "**Note**: start the optimization with the weights you obtained training in the smaller subset, i.e. *not* from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ad8b2d67a68a5afcf4c8645d3070550",
     "grade": false,
     "grade_id": "cell-3ae2a65188e4ac74",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "First, create two new data loaders, one for training samples and one for validation samples. This time, they'll load data from the folders for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94d6c3dc3c25680c53c2f2d4aef85b9d",
     "grade": true,
     "grade_id": "cell-64eaa83780f5eac9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24564177b59ed4f9364b679ac26b32a4",
     "grade": false,
     "grade_id": "cell-f3f79586de42561b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Train your model using the full data. This optimization might take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27996ba3be6dddb4dfe2da1d0964f5ff",
     "grade": true,
     "grade_id": "cell-c7dd71a632b5f152",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0ab46f558fb4b49f877ca0bae45376b",
     "grade": false,
     "grade_id": "cell-b1861d3a543c6386",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "How does the model perform now when trained on the entire dataset, compared to when only trained on the smaller subset of data? Create one plot with the training accuracy and another with the validation accuracy of the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52bacfa672fbc7eca004c87d041e3411",
     "grade": true,
     "grade_id": "cell-ceaac6be60ce36a9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7491bacf0ed1bed79c0e19f799079f69",
     "grade": false,
     "grade_id": "cell-b38092b08c150e7d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "What can you conclude from these plots? Did you expect what you observe in the plots, explain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22d5b2529d702c64919bef4e02ca308c",
     "grade": true,
     "grade_id": "cell-694a3fbb7f081da8",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "616d9047ba0c30d8343e48ecc58bd4d0",
     "grade": false,
     "grade_id": "cell-5e1ddfbfceb4d194",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 6. Evaluation on test set (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de024b1d9746bc4b0bb2b76627ef0926",
     "grade": false,
     "grade_id": "cell-a97630bf5d85363f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we'll evaluate your final model, obtained in step 6, on the test set. As mentioned before, the samples in the test set are not labelled, so we can't compute any supervised performance metrics ourselves. \n",
    "\n",
    "As a bit of fun and to inspire some friendly competition you may instead submit it to Kaggle for evaluation.\n",
    "\n",
    "Compute the predictions for all samples in the test set according to your best model, and save it in a .csv file with the format expected by the competition.\n",
    "\n",
    "For the test data we need a slightly different dataset class, due to the lack of labels in the data.\n",
    "A more proper way to implement it would be to make a common class which handles both the train and test settings.\n",
    "Here, we'll just copy the train dataset class and make some modifications to ignore the labels.\n",
    "\n",
    "Hints:\n",
    "- There is a `sampleSubmission.csv` file included in the zip data. Take a look at it to better understand what is the expected format here.\n",
    "- If you don't know how to create and write to files with Python, it's a well-behaved Google search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bc2a89934cfa251aef541415e610ab9",
     "grade": true,
     "grade_id": "cell-cc77ac7849f856e1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from time import time\n",
    "\n",
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, root: Path, transform):\n",
    "        root = Path(root)\n",
    "        if not (root.exists() and root.is_dir()):\n",
    "            raise ValueError(f\"Data root '{root}' is invalid\")\n",
    "            \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self._samples = self._collect_samples()\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        path = self._samples[index]\n",
    "        num_id = int(path.stem)\n",
    "        img = Image.open(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img) \n",
    "        return img, num_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "    def _collect_samples(self):\n",
    "        test_paths = self._collect_imgs_sub_dir(self.root)\n",
    "        return sorted(list(test_paths), key=lambda path: int(path.stem))\n",
    "     \n",
    "    @staticmethod\n",
    "    def _collect_imgs_sub_dir(sub_dir: Path):\n",
    "        if not sub_dir.exists():\n",
    "            raise ValueError(f\"Data root '{self.root}' must contain sub dir '{sub_dir.name}'\")\n",
    "        return sub_dir.glob(\"*.jpg\")\n",
    "    \n",
    "    def get_sample_by_id(self, id_):\n",
    "        id_index = self._samples.index(id_)\n",
    "        return self[id_index]\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97877f48922ebbd0c50829231a227ce5",
     "grade": false,
     "grade_id": "cell-faf8664f26ff7f4e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now that you created your submission file, submit it to Kaggle for evaluation. The [old competition](https://www.kaggle.com/c/dogs-vs-cats) does not allow submissions any more, but you can submit your file to the [new one](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition) via the \"Late submission\" button (they use the same data). The Kaggle CLI can be used as well. Kaggle evaluates your submission according to your log-loss score. Which score did you obtain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8507722245d56a20dd6809091664f78",
     "grade": true,
     "grade_id": "cell-e951dcec64dec85d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8a7f3a8236f43994efe29067d7237c2",
     "grade": false,
     "grade_id": "cell-dc362abcfef32eae",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "What was the username you used for this submission?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8cc61665c676edcd9192df3c15714aa3",
     "grade": true,
     "grade_id": "cell-d519532bb1f957c3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
